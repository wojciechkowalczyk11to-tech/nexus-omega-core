# üìö NexusOmegaCore ‚Äî Dokumentacja Provider√≥w AI

> Kompleksowy opis wszystkich provider√≥w AI, modeli, funkcji, cennika i statusu operacyjnego.
>
> Ostatnia aktualizacja: 2026-02-21

---

## üìã Spis tre≈õci

1. [PrzeglƒÖd architektury provider√≥w](#przeglƒÖd-architektury-provider√≥w)
2. [Tabela zbiorcza provider√≥w](#tabela-zbiorcza-provider√≥w)
3. [Szczeg√≥≈Çowy opis provider√≥w](#szczeg√≥≈Çowy-opis-provider√≥w)
   - [Google Gemini](#1-google-gemini)
   - [DeepSeek](#2-deepseek)
   - [Groq](#3-groq)
   - [OpenRouter](#4-openrouter)
   - [xAI Grok](#5-xai-grok)
   - [OpenAI](#6-openai)
   - [Anthropic Claude](#7-anthropic-claude)
4. [System profili (ECO / SMART / DEEP)](#system-profili)
5. [SLM Router ‚Äî routing kosztowy](#slm-router)
6. [Fallback chain ‚Äî ≈Ça≈Ñcuch awaryjny](#fallback-chain)
7. [RBAC ‚Äî dostƒôp do provider√≥w](#rbac--dostƒôp-do-provider√≥w)
8. [Narzƒôdzia (Tools)](#narzƒôdzia-tools)
9. [Konfiguracja i zmienne ≈õrodowiskowe](#konfiguracja)
10. [Status operacyjny i healthcheck](#status-operacyjny)
11. [FAQ](#faq)

---

## PrzeglƒÖd architektury provider√≥w

NexusOmegaCore wykorzystuje architekturƒô **multi-provider** z automatycznym routingiem i fallbackiem. Ka≈ºdy provider AI implementuje wsp√≥lny interfejs `BaseProvider`, co zapewnia jednolity spos√≥b wywo≈Çywania generacji tekstu niezale≈ºnie od dostawcy.

### Schemat przep≈Çywu zapytania

```
U≈ºytkownik ‚Üí Telegram Bot ‚Üí Backend API ‚Üí PolicyEngine (RBAC)
    ‚Üí ModelRouter (klasyfikacja trudno≈õci)
    ‚Üí ProviderFactory (tworzenie instancji providera)
    ‚Üí Provider.generate() (wywo≈Çanie API)
    ‚Üí ProviderResponse (ustandaryzowana odpowied≈∫)
    ‚Üí UsageService (rozliczenie koszt√≥w)
    ‚Üí Odpowied≈∫ do u≈ºytkownika
```

### Interfejs BaseProvider

Ka≈ºdy provider implementuje nastƒôpujƒÖce metody:

| Metoda | Opis |
|--------|------|
| `generate(messages, model, temperature, max_tokens)` | Generacja odpowiedzi z listy wiadomo≈õci |
| `get_model_for_profile(profile)` | Dob√≥r modelu dla profilu (eco/smart/deep) |
| `calculate_cost(model, input_tokens, output_tokens)` | Kalkulacja kosztu w USD |
| `is_available()` | Sprawdzenie czy provider ma klucz API |
| `name` | Identyfikator providera (np. `"gemini"`) |
| `display_name` | Nazwa wy≈õwietlana (np. `"Google Gemini"`) |

### Standardowa odpowied≈∫ (ProviderResponse)

Ka≈ºdy provider zwraca obiekt `ProviderResponse` ze standardowymi polami:

| Pole | Typ | Opis |
|------|-----|------|
| `content` | `str` | Wygenerowana tre≈õƒá odpowiedzi |
| `model` | `str` | Identyfikator u≈ºytego modelu |
| `input_tokens` | `int` | Liczba token√≥w wej≈õciowych |
| `output_tokens` | `int` | Liczba token√≥w wyj≈õciowych |
| `cost_usd` | `float` | Koszt zapytania w USD |
| `latency_ms` | `int` | Czas odpowiedzi w milisekundach |
| `finish_reason` | `str` | Pow√≥d zako≈Ñczenia (np. `"stop"`) |
| `raw_response` | `dict` | Surowa odpowied≈∫ z API providera |

---

## Tabela zbiorcza provider√≥w

| # | Provider | Modele | Tier cenowy | Dostƒôp DEMO | Dostƒôp FULL | Klucz API |
|---|----------|--------|-------------|-------------|-------------|-----------|
| 1 | **Google Gemini** | gemini-2.0-flash-exp, gemini-2.0-flash-thinking-exp-1219, gemini-exp-1206 | Darmowy (free tier) | ‚úÖ | ‚úÖ | `GEMINI_API_KEY` |
| 2 | **DeepSeek** | deepseek-chat, deepseek-reasoner | Bardzo tani | ‚úÖ (50/dzie≈Ñ) | ‚úÖ | `DEEPSEEK_API_KEY` |
| 3 | **Groq** | llama-3.3-70b-versatile | Darmowy | ‚úÖ | ‚úÖ | `GROQ_API_KEY` |
| 4 | **OpenRouter** | llama-3.2-3b-instruct:free, llama-3.1-8b-instruct:free | Darmowy (free tier) | ‚úÖ | ‚úÖ | `OPENROUTER_API_KEY` |
| 5 | **xAI Grok** | grok-beta, grok-2-latest | Premium | ‚úÖ (5/dzie≈Ñ) | ‚úÖ | `XAI_API_KEY` |
| 6 | **OpenAI** | gpt-4o-mini, gpt-4o, gpt-4-turbo | ≈öredni‚ÄìPremium | ‚ùå | ‚úÖ | `OPENAI_API_KEY` |
| 7 | **Anthropic Claude** | claude-3-5-haiku, claude-3-5-sonnet, claude-3-opus | ≈öredni‚ÄìPremium | ‚ùå | ‚úÖ | `ANTHROPIC_API_KEY` |

---

## Szczeg√≥≈Çowy opis provider√≥w

### 1. Google Gemini

**Klasa:** `GeminiProvider`
**Plik:** `backend/app/providers/gemini_provider.py`
**Biblioteka:** `google-generativeai`

#### Opis

Google Gemini to g≈Ç√≥wny provider do zada≈Ñ ekonomicznych (profil ECO). Oferuje darmowe modele eksperymentalne z du≈ºym oknem kontekstowym (do 2M token√≥w). Provider konwertuje wiadomo≈õci z formatu OpenAI na format Gemini (mapowanie r√≥l: `system` ‚Üí `user [System]`, `assistant` ‚Üí `model`).

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `gemini-2.0-flash-exp` | ECO | 1 000 000 | $0.00 | $0.00 | Darmowy, szybki, eksperymentalny |
| `gemini-2.0-flash-thinking-exp-1219` | SMART | 32 000 | $0.00 | $0.00 | Darmowy, z reasoning |
| `gemini-exp-1206` | DEEP | 2 000 000 | $0.00 | $0.00 | Darmowy, najwiƒôksze okno kontekstu |
| `gemini-1.5-flash` | (dodatkowy) | 1 000 000 | $0.075 | $0.30 | P≈Çatny, produkcyjny |
| `gemini-1.5-pro` | (dodatkowy) | 2 000 000 | $1.25 | $5.00 | P≈Çatny, najwy≈ºsza jako≈õƒá |

#### Funkcje

- ‚úÖ Generacja tekstu (chat completions)
- ‚úÖ Konwersja wiadomo≈õci systemowych na format Gemini
- ‚úÖ Fallback estimacja token√≥w (gdy brak usage_metadata)
- ‚úÖ Asynchroniczne wywo≈Çania przez `run_in_executor`
- ‚úÖ Konfigurowalny temperature i max_tokens
- ‚ùå Brak natywnego wsparcia dla system prompt (emulowany)

#### Specyfika implementacji

- U≈ºywa `genai.GenerativeModel` z synchronicznym `generate_content` opakowanym w `loop.run_in_executor` dla async kompatybilno≈õci
- Rola `system` jest mapowana na `user` z prefiksem `[System]`
- Rola `assistant` jest mapowana na `model`

---

### 2. DeepSeek

**Klasa:** `DeepSeekProvider`
**Plik:** `backend/app/providers/deepseek_provider.py`
**Biblioteka:** `openai` (kompatybilne API)

#### Opis

DeepSeek to chi≈Ñski provider AI oferujƒÖcy modele o bardzo niskim koszcie. Wykorzystuje API kompatybilne z OpenAI (`base_url: https://api.deepseek.com`). Jest g≈Ç√≥wnym providerem dla profilu SMART i DEEP ze wzglƒôdu na doskona≈Çy stosunek jako≈õci do ceny.

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `deepseek-chat` | ECO | 64 000 | $0.14 | $0.28 | Szybki, ekonomiczny |
| `deepseek-reasoner` | SMART, DEEP | 64 000 | $0.55 | $2.19 | Z reasoning (chain-of-thought) |

#### Funkcje

- ‚úÖ Pe≈Çna kompatybilno≈õƒá z OpenAI API
- ‚úÖ Chat completions
- ‚úÖ Reasoning (deepseek-reasoner)
- ‚úÖ System prompts
- ‚úÖ Precyzyjne usage tracking (prompt_tokens, completion_tokens)

---

### 3. Groq

**Klasa:** `GroqProvider`
**Plik:** `backend/app/providers/groq_provider.py`
**Biblioteka:** `openai` (kompatybilne API)

#### Opis

Groq to provider specjalizujƒÖcy siƒô w ultra-szybkim inferenzie na dedykowanym hardware (LPU‚Ñ¢). Oferuje darmowy tier z modelami Llama. Wykorzystuje API kompatybilne z OpenAI (`base_url: https://api.groq.com/openai/v1`).

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `llama-3.3-70b-versatile` | ECO, SMART, DEEP | 128 000 | $0.00 | $0.00 | Darmowy, Llama 3.3 70B |
| `llama-3.1-70b-versatile` | (dodatkowy) | 128 000 | $0.00 | $0.00 | Darmowy, Llama 3.1 70B |

#### Funkcje

- ‚úÖ Ultra-szybki inference (LPU hardware)
- ‚úÖ Darmowy tier
- ‚úÖ Pe≈Çna kompatybilno≈õƒá z OpenAI API
- ‚úÖ Chat completions
- ‚úÖ System prompts
- ‚ö†Ô∏è Rate limiting na darmowym tierze

#### Specyfika implementacji

- Metoda `calculate_cost()` zawsze zwraca `0.0` (darmowy tier)
- Idealny jako fallback provider w ≈Ça≈Ñcuchu awaryjnym

---

### 4. OpenRouter

**Klasa:** `OpenRouterProvider`
**Plik:** `backend/app/providers/openrouter_provider.py`
**Biblioteka:** `openai` (kompatybilne API)

#### Opis

OpenRouter to agregator modeli AI oferujƒÖcy dostƒôp do wielu modeli przez jedno API. NexusOmegaCore u≈ºywa wy≈ÇƒÖcznie darmowych modeli z free tier. Wykorzystuje API kompatybilne z OpenAI (`base_url: https://openrouter.ai/api/v1`).

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `meta-llama/llama-3.2-3b-instruct:free` | ECO | 64 000 | $0.00 | $0.00 | Darmowy, ma≈Çy model |
| `meta-llama/llama-3.1-8b-instruct:free` | SMART, DEEP | 64 000 | $0.00 | $0.00 | Darmowy, ≈õredni model |

#### Funkcje

- ‚úÖ Dostƒôp do wielu modeli przez jedno API
- ‚úÖ Darmowy tier
- ‚úÖ Pe≈Çna kompatybilno≈õƒá z OpenAI API
- ‚úÖ Chat completions
- ‚ö†Ô∏è Rate limiting na darmowym tierze
- ‚ö†Ô∏è Ograniczona jako≈õƒá na darmowych modelach

---

### 5. xAI Grok

**Klasa:** `GrokProvider`
**Plik:** `backend/app/providers/grok_provider.py`
**Biblioteka:** `openai` (kompatybilne API)

#### Opis

Grok to model AI od xAI (firmy Elona Muska). Oferuje zaawansowane mo≈ºliwo≈õci konwersacyjne. Wykorzystuje API kompatybilne z OpenAI (`base_url: https://api.x.ai/v1`). Jest providerem premium z limitem 5 zapyta≈Ñ dziennie dla u≈ºytkownik√≥w DEMO.

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `grok-beta` | ECO, SMART, DEEP | 128 000 | $5.00 | $15.00 | Premium, beta |
| `grok-2-latest` | (dodatkowy) | 128 000 | $5.00 | $15.00 | Premium, stabilny |

#### Funkcje

- ‚úÖ Zaawansowany reasoning
- ‚úÖ Pe≈Çna kompatybilno≈õƒá z OpenAI API
- ‚úÖ Chat completions
- ‚úÖ System prompts
- ‚ö†Ô∏è Limit 5/dzie≈Ñ dla DEMO u≈ºytkownik√≥w
- ‚ö†Ô∏è Wysoki koszt ($5-15 / 1M token√≥w)

---

### 6. OpenAI

**Klasa:** `OpenAIProvider`
**Plik:** `backend/app/providers/openai_provider.py`
**Biblioteka:** `openai`

#### Opis

OpenAI GPT to flagowy provider dla zada≈Ñ wymagajƒÖcych najwy≈ºszej jako≈õci. Dostƒôpny wy≈ÇƒÖcznie dla u≈ºytkownik√≥w z rolƒÖ FULL_ACCESS lub ADMIN. Oferuje modele od ekonomicznego GPT-4o-mini po zaawansowany GPT-4-turbo.

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `gpt-4o-mini` | ECO | 128 000 | $0.15 | $0.60 | Ekonomiczny, szybki |
| `gpt-4o` | SMART, DEEP | 128 000 | $2.50 | $10.00 | Balans jako≈õƒá/koszt |
| `gpt-4-turbo` | (dodatkowy) | 128 000 | $10.00 | $30.00 | Najwy≈ºsza jako≈õƒá |

#### Funkcje

- ‚úÖ Natywne API OpenAI (AsyncOpenAI)
- ‚úÖ Chat completions
- ‚úÖ System prompts
- ‚úÖ Function calling
- ‚úÖ Precyzyjne usage tracking
- ‚úÖ Streaming (obs≈Çugiwane przez bibliotekƒô)
- ‚ùå Dostƒôp tylko dla FULL_ACCESS / ADMIN

---

### 7. Anthropic Claude

**Klasa:** `ClaudeProvider`
**Plik:** `backend/app/providers/claude_provider.py`
**Biblioteka:** `anthropic`

#### Opis

Anthropic Claude to provider premium z dedykowanym API. Specjalizuje siƒô w zadaniach analitycznych, d≈Çugich konwersacjach i kodowaniu. Dostƒôpny wy≈ÇƒÖcznie dla u≈ºytkownik√≥w FULL_ACCESS / ADMIN. Implementacja wyodrƒôbnia system prompt z listy wiadomo≈õci i przekazuje go osobno (specyfika API Anthropic).

#### Modele

| Model | Profil | Okno kontekstu | Koszt input / 1M token√≥w | Koszt output / 1M token√≥w | Uwagi |
|-------|--------|----------------|---------------------------|----------------------------|-------|
| `claude-3-5-haiku-20241022` | ECO | 200 000 | $0.80 | $4.00 | Szybki, ekonomiczny |
| `claude-3-5-sonnet-20241022` | SMART, DEEP | 200 000 | $3.00 | $15.00 | Balans jako≈õƒá/koszt |
| `claude-3-opus-20240229` | (dodatkowy) | 200 000 | $15.00 | $75.00 | Najwy≈ºsza jako≈õƒá |

#### Funkcje

- ‚úÖ Dedykowane API Anthropic (AsyncAnthropic)
- ‚úÖ Separacja system prompt (natywna obs≈Çuga)
- ‚úÖ Chat completions
- ‚úÖ Du≈ºe okno kontekstu (200K token√≥w)
- ‚úÖ Function calling (tool use)
- ‚úÖ Precyzyjne usage tracking
- ‚ùå Dostƒôp tylko dla FULL_ACCESS / ADMIN

#### Specyfika implementacji

- System message jest wyodrƒôbniany z listy wiadomo≈õci i przekazywany jako osobny parametr `system` w ≈ºƒÖdaniu API
- Pozosta≈Çe wiadomo≈õci (user/assistant) sƒÖ przekazywane normalnie

---

## System profili

NexusOmegaCore implementuje trzy profile jako≈õciowe, kt√≥re wp≈ÇywajƒÖ na dob√≥r modelu u ka≈ºdego providera:

### ECO (ekonomiczny)

- **Cel:** Minimalizacja koszt√≥w
- **U≈ºycie:** Proste pytania, szybkie odpowiedzi
- **Klasyfikacja:** Zapytania ≈Çatwe (DifficultyLevel.EASY)
- **≈Åa≈Ñcuch provider√≥w:** Gemini ‚Üí Groq ‚Üí DeepSeek

| Provider | Model ECO |
|----------|-----------|
| Gemini | `gemini-2.0-flash-exp` |
| DeepSeek | `deepseek-chat` |
| Groq | `llama-3.3-70b-versatile` |
| OpenRouter | `meta-llama/llama-3.2-3b-instruct:free` |
| Grok | `grok-beta` |
| OpenAI | `gpt-4o-mini` |
| Claude | `claude-3-5-haiku-20241022` |

### SMART (zbalansowany)

- **Cel:** Balans miƒôdzy jako≈õciƒÖ a kosztem
- **U≈ºycie:** Pytania ≈õredniej trudno≈õci, wyja≈õnienia, kod
- **Klasyfikacja:** Zapytania ≈õrednie (DifficultyLevel.MEDIUM)
- **≈Åa≈Ñcuch provider√≥w:** DeepSeek ‚Üí Gemini ‚Üí Groq

| Provider | Model SMART |
|----------|------------|
| Gemini | `gemini-2.0-flash-thinking-exp-1219` |
| DeepSeek | `deepseek-reasoner` |
| Groq | `llama-3.3-70b-versatile` |
| OpenRouter | `meta-llama/llama-3.1-8b-instruct:free` |
| Grok | `grok-beta` |
| OpenAI | `gpt-4o` |
| Claude | `claude-3-5-sonnet-20241022` |

### DEEP (premium)

- **Cel:** Maksymalna jako≈õƒá odpowiedzi
- **U≈ºycie:** Z≈Ço≈ºone analizy, architektura, optymalizacja
- **Klasyfikacja:** Zapytania trudne (DifficultyLevel.HARD)
- **≈Åa≈Ñcuch provider√≥w:** DeepSeek ‚Üí Gemini ‚Üí OpenAI ‚Üí Claude
- **Dostƒôp:** Tylko FULL_ACCESS i ADMIN

| Provider | Model DEEP |
|----------|-----------|
| Gemini | `gemini-exp-1206` |
| DeepSeek | `deepseek-reasoner` |
| Groq | `llama-3.3-70b-versatile` |
| OpenRouter | `meta-llama/llama-3.1-8b-instruct:free` |
| Grok | `grok-beta` |
| OpenAI | `gpt-4o` |
| Claude | `claude-3-5-sonnet-20241022` |

---

## SLM Router

System **SLM-first Cost-Aware Router** preferuje ma≈Çe, tanie modele i eskaluje tylko gdy jest to konieczne.

### Tiery modeli

| Tier | Koszt ~/ 1M token√≥w | Modele | Prƒôdko≈õƒá |
|------|---------------------|--------|----------|
| **ULTRA_CHEAP** | ~$0.10 | Groq Llama 3.1 8B, Gemini Flash | ‚ö°‚ö°‚ö° |
| **CHEAP** | ~$0.50 | DeepSeek Chat, Gemini 1.5 Pro | ‚ö°‚ö° |
| **BALANCED** | ~$2.00 | GPT-4o-mini, Claude Sonnet | ‚ö° |
| **PREMIUM** | ~$10.00+ | GPT-4-turbo, Claude Opus | üêå |

### Logika eskalacji

1. Rozpoczyna od **ULTRA_CHEAP** tier
2. Je≈õli `cost_preference = LOW` ‚Üí obni≈ºa tier o 1
3. Je≈õli `cost_preference = QUALITY` ‚Üí podnosi tier o 1
4. Je≈õli brak pasujƒÖcych modeli ‚Üí eskaluje do nastƒôpnego tieru
5. Heurystyka `should_escalate()` por√≥wnuje `task_complexity_score` z `model_capability`

### Preferencje kosztowe u≈ºytkownika

| Preferencja | Opis | Domy≈õlny tier dla "moderate" |
|-------------|------|------------------------------|
| `LOW` | Minimalizuj koszt | ULTRA_CHEAP |
| `BALANCED` | Balans | CHEAP |
| `QUALITY` | Priorytet jako≈õƒá | BALANCED |

---

## Fallback chain

System automatycznie przechodzi do nastƒôpnego providera w ≈Ça≈Ñcuchu je≈õli bie≈ºƒÖcy zawiedzie.

### ≈Åa≈Ñcuchy awaryjne

```
ECO:   gemini ‚Üí groq ‚Üí deepseek
SMART: deepseek ‚Üí gemini ‚Üí groq
DEEP:  deepseek ‚Üí gemini ‚Üí openai ‚Üí claude
```

### Mechanizm

1. `ProviderFactory.generate_with_fallback()` iteruje po ≈Ça≈Ñcuchu
2. Dla ka≈ºdego providera: tworzy instancjƒô ‚Üí dobiera model ‚Üí generuje
3. Je≈õli `ProviderError` ‚Üí loguje ostrze≈ºenie ‚Üí pr√≥buje nastƒôpnego
4. Je≈õli wszystkie zawiodƒÖ ‚Üí rzuca `AllProvidersFailedError`
5. Zwraca tuple: `(ProviderResponse, provider_name, fallback_used)`

### Filtrowanie ≈Ça≈Ñcucha po RBAC

≈Åa≈Ñcuch jest filtrowany na podstawie roli u≈ºytkownika. Np. u≈ºytkownik DEMO z profilem DEEP dostanie ≈Ça≈Ñcuch `deepseek ‚Üí gemini` (bez `openai` i `claude`).

---

## RBAC ‚Äî dostƒôp do provider√≥w

### Macierz dostƒôpu

| Provider | DEMO | FULL_ACCESS | ADMIN |
|----------|------|-------------|-------|
| Gemini | ‚úÖ | ‚úÖ | ‚úÖ |
| DeepSeek | ‚úÖ (50/dzie≈Ñ) | ‚úÖ | ‚úÖ |
| Groq | ‚úÖ | ‚úÖ | ‚úÖ |
| OpenRouter | ‚úÖ | ‚úÖ | ‚úÖ |
| Grok | ‚úÖ (5/dzie≈Ñ) | ‚úÖ | ‚úÖ |
| OpenAI | ‚ùå | ‚úÖ | ‚úÖ |
| Claude | ‚ùå | ‚úÖ | ‚úÖ |

### Limity dzienne (DEMO)

| Zas√≥b | Limit dzienny |
|-------|---------------|
| Grok calls | 5 |
| Web search calls | 5 |
| Smart credits | 20 |
| DeepSeek calls | 50 |

### Bud≈ºet dzienny (USD)

| Rola | Bud≈ºet |
|------|--------|
| DEMO | $0.00 |
| FULL_ACCESS | $5.00 |
| ADMIN | Bez limitu |

### Smart Credits

Kalkulacja na podstawie ≈ÇƒÖcznej liczby token√≥w:

| Tokeny | Kredyty |
|--------|---------|
| ‚â§ 500 | 1 |
| ‚â§ 2000 | 2 |
| > 2000 | 4 |

---

## Narzƒôdzia (Tools)

Opr√≥cz provider√≥w AI, system oferuje zestaw narzƒôdzi zintegrowanych z ReAct Orchestratorem:

### Dostƒôpne narzƒôdzia

| Narzƒôdzie | Opis | Wymagany klucz API |
|-----------|------|---------------------|
| **Web Search** | Wyszukiwanie w internecie (Brave Search API) | `BRAVE_SEARCH_API_KEY` |
| **Vertex AI Search** | Wyszukiwanie w bazie wiedzy z cytatami | `VERTEX_PROJECT_ID`, `VERTEX_SEARCH_DATASTORE_ID` |
| **RAG Search** | Wyszukiwanie semantyczne w dokumentach u≈ºytkownika | ‚Äî (wbudowane) |
| **Calculate** | Obliczenia matematyczne (safe eval) | ‚Äî |
| **Get DateTime** | Pobranie aktualnej daty i czasu | ‚Äî |
| **Memory Read/Write** | Odczyt/zapis do pamiƒôci absolutnej u≈ºytkownika | ‚Äî |
| **GitHub Devin** | Klonowanie repo, edycja kodu, tworzenie PR | `GITHUB_TOKEN` |

### ReAct Orchestrator

System **ReAct (Reason-Act-Observe-Think)** zarzƒÖdza pƒôtlƒÖ narzƒôdziowƒÖ:

1. **REASON** ‚Äî LLM analizuje zapytanie i decyduje o u≈ºyciu narzƒôdzia
2. **ACT** ‚Äî Wykonanie narzƒôdzia lub generacja odpowiedzi
3. **OBSERVE** ‚Äî Analiza wyniku narzƒôdzia
4. **THINK** ‚Äî Self-correction: czy wynik jest poprawny?
5. **RESPOND** ‚Äî Finalna odpowied≈∫ do u≈ºytkownika

- Maks. 6 iteracji pƒôtli ReAct
- Maks. 2 self-corrections na iteracjƒô

### Token Budget Manager

System inteligentnego zarzƒÖdzania bud≈ºetem token√≥w:

- **Priorytetyzacja:** System prompt (90) > Bie≈ºƒÖce zapytanie (100) > Pamiƒôƒá (65) > Historia (30-40) > Snapshot (10)
- **Smart truncation:** Zachowuje pierwszy i ostatni akapit, skraca ≈õrodek
- **Rezerwa na odpowied≈∫:** 15% kontekstu
- **Margines bezpiecze≈Ñstwa:** 5% kontekstu

---

## Konfiguracja

### Zmienne ≈õrodowiskowe provider√≥w

```env
# Wymagany (minimum 1 provider)
GEMINI_API_KEY=         # Google Gemini ‚Äî g≈Ç√≥wny darmowy provider
DEEPSEEK_API_KEY=       # DeepSeek ‚Äî tani, dobry reasoning
GROQ_API_KEY=           # Groq ‚Äî darmowy, ultra-szybki

# Opcjonalne (free tier)
OPENROUTER_API_KEY=     # OpenRouter ‚Äî agregator darmowych modeli

# Opcjonalne (premium)
XAI_API_KEY=            # xAI Grok ‚Äî premium conversational AI
OPENAI_API_KEY=         # OpenAI GPT ‚Äî tylko FULL_ACCESS/ADMIN
ANTHROPIC_API_KEY=      # Anthropic Claude ‚Äî tylko FULL_ACCESS/ADMIN
```

### Konfiguracja policy (JSON)

```env
PROVIDER_POLICY_JSON={"default":{"providers":{"gemini":{"enabled":true},"deepseek":{"enabled":true},"groq":{"enabled":true}}}}
```

### Aliasy provider√≥w

System automatycznie normalizuje nazwy provider√≥w:

| Alias | Mapowany na |
|-------|-------------|
| `xai`, `x.ai` | `grok` |
| `google` | `gemini` |
| `anthropic` | `claude` |
| `llama` | `groq` |

---

## Status operacyjny

### Healthcheck

```bash
curl http://localhost:8000/api/v1/health
```

Oczekiwana odpowied≈∫:
```json
{
  "status": "healthy",
  "database": "healthy",
  "redis": "healthy"
}
```

### Sprawdzenie dostƒôpnych provider√≥w

```bash
curl -H "Authorization: Bearer <token>" http://localhost:8000/api/v1/chat/providers
```

### Monitoring

- **Logi:** JSON format z request tracing (`LOG_JSON=true`)
- **Usage tracking:** Tabela `usage_ledger` z pe≈Çnym rozliczeniem koszt√≥w
- **Tool counters:** Tabela `tool_counters` ze zliczaniem u≈ºycia dziennego
- **Audit log:** Tabela `audit_logs` z akcjami administracyjnymi
- **Agent traces:** Tabela `agent_traces` z pe≈Çnym ≈õladem rozumowania ReAct

### Wdro≈ºenie na VM

1. Zainstaluj Docker i Docker Compose na VM
2. Sklonuj repozytorium: `git clone <repo_url>`
3. Skopiuj `.env.example` do `.env` i wype≈Çnij klucze API
4. Uruchom: `docker compose -f docker-compose.production.yml up -d`
5. Sprawd≈∫ healthcheck: `curl http://localhost:8000/api/v1/health`
6. Zweryfikuj bota w Telegram: `/start`

Szczeg√≥≈Çowe instrukcje w [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md).

---

## FAQ

### Kt√≥ry provider wybraƒá jako minimum?

**Gemini** ‚Äî darmowy, du≈ºe okno kontekstu, dobra jako≈õƒá. Wystarczy jako jedyny provider.

### Ile kosztuje typowe zapytanie?

| Profil | Provider | ~Koszt na zapytanie (1K in / 500 out) |
|--------|----------|---------------------------------------|
| ECO | Gemini | $0.00 (darmowy) |
| ECO | Groq | $0.00 (darmowy) |
| SMART | DeepSeek | ~$0.001 |
| SMART | OpenAI (gpt-4o) | ~$0.008 |
| DEEP | Claude Sonnet | ~$0.011 |
| DEEP | Claude Opus | ~$0.053 |

### Jak dodaƒá nowego providera?

1. Utw√≥rz plik `backend/app/providers/<name>_provider.py`
2. Zaimplementuj klasƒô dziedziczƒÖcƒÖ z `BaseProvider`
3. Zarejestruj w `ProviderFactory.PROVIDERS` i `_get_api_key()`
4. Dodaj klucz API do `Settings` w `core/config.py`
5. Dodaj do `.env.example`
6. Zaktualizuj `PolicyEngine.PROVIDER_ACCESS` dla r√≥l

### Co siƒô stanie gdy provider zawiedzie?

System automatycznie przejdzie do nastƒôpnego providera w ≈Ça≈Ñcuchu awaryjnym. Je≈õli wszystkie zawiodƒÖ, u≈ºytkownik otrzyma komunikat: *"Wszystkie providery AI zawiod≈Çy. Spr√≥buj ponownie p√≥≈∫niej."*

### Jak dzia≈Ça klasyfikacja trudno≈õci?

Wielosygna≈Çowy scoring:
1. **S≈Çowa kluczowe** (PL + EN) ‚Äî waga 40% (hard) / 30% (medium)
2. **Z≈Ço≈ºono≈õƒá strukturalna** ‚Äî waga 50% (d≈Çugo≈õƒá, bloki kodu, listy)
3. **Detekcja intencji** ‚Äî bonus 30% (analityczne, kod)
4. **Scoring:** ‚â•0.5 = HARD, ‚â•0.15 = MEDIUM, <0.15 = EASY

---

*Dokument wygenerowany automatycznie na podstawie kodu ≈∫r√≥d≈Çowego NexusOmegaCore.*
